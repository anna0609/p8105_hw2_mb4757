---
title: "Homework 2"
author: Minjie Bao
output: github_document
---

```{r setup}
library(tidyverse)
library(readxl)
```

## Problem 1

Read the Mr. Trashwheel dataset.

```{r}
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
         )
```

Read precipitation data! For 2018 and 2017.

```{r}
precip_2018 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2018 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)

precip_2017 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2017 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now combine annual precipitation.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )
precip_df = 
  bind_rows(precip_2018, precip_2017)

left_join(precip_df, month_df, by = "month")
```

This dataset contains information from the Mr. Trashwheel trash collector in Baltimore, Maryland. As trash enters the inner harbor, the trashwheel collects that trash, and stores it in a dumpster. The dataset contains information on year, month, and trash collected, include some specific kinds of trash. There are a total of `r nrow(trashwheel_df)` rows in our final dataset. Additional data sheets include month precipitation data.


## Problem 2
Read and clean the data, covert entry from char to logical
```{r}
nyc_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") 
nyc_df = janitor::clean_names(nyc_df)
nyc_df = select (nyc_df, line, station_name, station_latitude, station_longitude, entry, route1:route11, vending, entrance_type, ada)
nyc_df = mutate(nyc_df, entry = recode(entry, "YES" = "TRUE", "NO" = "FALSE"), entry = as.logical(entry))
skimr::skim(nyc_df)
```

This dataset contains information from the NYC Transit data; in particular, it has information related to each entrance and exit for each subway station in NYC. There are 19 variables in this dataset includes line, station name, entry and route served etc. For data cleaning steps, I read the data first and usejanitor to clean the variables' name. Then I change the variable entry from character to logical. Finally, I use skimr to take a brief view of the descriptive statistics of the dataset. The dimention(rows x columns) of the dataset is `r nrow(nyc_df)` * `r ncol(nyc_df)`. The data is tidy since columns are variables, rows are observations and every value has a cell.

```{r}
distinct_data1 = distinct(nyc_df, line, station_name, .keep_all = TRUE)
```
There are 465 distinct stations here.

```{r}
filter(distinct_data1,  ada == "TRUE")
```
There are 84 stations are ADA compliant.

```{r}
filter(nyc_df,  vending == "NO")
a = nrow(filter(nyc_df,  vending == "NO"))
b = nrow(nyc_df)
a/b ##The proportion of station entrances / exits without vending allow entrance
```
The proportion of station entrances / exits without vending allow entrance is 0.0979657.

convert variable route9-route11 from double to char.
```{r}
nyc_df_char = mutate(nyc_df, route8 = as.character(route8),
                route9 = as.character(route9),
                route10 = as.character(route10),
                route11 = as.character(route11)
                )
```

reform the data.
```{r}
nyc_df_tidy =
  nyc_df_char %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  )
```

```{r}
distinct_data2 = distinct(nyc_df_tidy, line, station_name, .keep_all = TRUE)
filter(distinct_data2,  route_name == "A")
```
There are 60 distinct station.

```{r}
filter(distinct_data2,  route_name == "A", ada == "TRUE")
```
There are 17 ada compliant of the stations serve the A train.


## Problem 3
read the csv datasets
```{r}

```


