Homework 2
================
Minjie Bao

``` r
library(tidyverse)
library(readxl)
```

## Problem 1

Read the Mr. Trashwheel dataset.

``` r
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
         )
```

Read precipitation data\! For 2018 and 2017.

``` r
precip_2018 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2018 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)

precip_2017 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2017 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now combine annual precipitation.

``` r
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )
precip_df = 
  bind_rows(precip_2018, precip_2017)

left_join(precip_df, month_df, by = "month")
```

    ## # A tibble: 24 x 4
    ##     year month total month_name
    ##    <dbl> <dbl> <dbl> <chr>     
    ##  1  2018     1  0.94 January   
    ##  2  2018     2  4.8  February  
    ##  3  2018     3  2.69 March     
    ##  4  2018     4  4.69 April     
    ##  5  2018     5  9.27 May       
    ##  6  2018     6  4.77 June      
    ##  7  2018     7 10.2  July      
    ##  8  2018     8  6.45 August    
    ##  9  2018     9 10.5  September 
    ## 10  2018    10  2.12 October   
    ## # … with 14 more rows

This dataset contains information from the Mr. Trashwheel trash
collector in Baltimore, Maryland. As trash enters the inner harbor, the
trashwheel collects that trash, and stores it in a dumpster. The dataset
contains information on year, month, and trash collected, include some
specific kinds of trash. There are a total of 344 rows in our final
dataset. Additional data sheets include month precipitation data.

## Problem 2

Read and clean the data, covert entry variable from char to logical

``` r
nyc_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select (line, station_name, station_latitude, station_longitude, entry, route1:route11, vending, entrance_type, ada) %>% 
  mutate(entry = recode(entry, "YES" = "TRUE", "NO" = "FALSE"), entry = as.logical(entry))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
skimr::skim(nyc_df)
```

|                                                  |         |
| :----------------------------------------------- | :------ |
| Name                                             | nyc\_df |
| Number of rows                                   | 1868    |
| Number of columns                                | 19      |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |         |
| Column type frequency:                           |         |
| character                                        | 11      |
| logical                                          | 2       |
| numeric                                          | 6       |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |         |
| Group variables                                  | None    |

Data summary

**Variable type: character**

| skim\_variable | n\_missing | complete\_rate | min | max | empty | n\_unique | whitespace |
| :------------- | ---------: | -------------: | --: | --: | ----: | --------: | ---------: |
| line           |          0 |           1.00 |   5 |  17 |     0 |        36 |          0 |
| station\_name  |          0 |           1.00 |   4 |  39 |     0 |       356 |          0 |
| route1         |          0 |           1.00 |   1 |   2 |     0 |        24 |          0 |
| route2         |        848 |           0.55 |   1 |   2 |     0 |        20 |          0 |
| route3         |       1374 |           0.26 |   1 |   2 |     0 |        18 |          0 |
| route4         |       1547 |           0.17 |   1 |   1 |     0 |        13 |          0 |
| route5         |       1630 |           0.13 |   1 |   1 |     0 |        12 |          0 |
| route6         |       1741 |           0.07 |   1 |   1 |     0 |         7 |          0 |
| route7         |       1788 |           0.04 |   1 |   2 |     0 |         7 |          0 |
| vending        |          0 |           1.00 |   2 |   3 |     0 |         2 |          0 |
| entrance\_type |          0 |           1.00 |   4 |   9 |     0 |         7 |          0 |

**Variable type: logical**

| skim\_variable | n\_missing | complete\_rate | mean | count               |
| :------------- | ---------: | -------------: | ---: | :------------------ |
| entry          |          0 |              1 | 0.94 | TRU: 1753, FAL: 115 |
| ada            |          0 |              1 | 0.25 | FAL: 1400, TRU: 468 |

**Variable type: numeric**

| skim\_variable     | n\_missing | complete\_rate |    mean |   sd |      p0 |     p25 |     p50 |     p75 |    p100 | hist  |
| :----------------- | ---------: | -------------: | ------: | ---: | ------: | ------: | ------: | ------: | ------: | :---- |
| station\_latitude  |          0 |           1.00 |   40.73 | 0.07 |   40.58 |   40.69 |   40.73 |   40.77 |   40.90 | ▂▅▇▃▂ |
| station\_longitude |          0 |           1.00 | \-73.94 | 0.06 | \-74.03 | \-73.99 | \-73.96 | \-73.91 | \-73.76 | ▇▆▃▂▁ |
| route8             |       1820 |           0.03 |    2.98 | 1.94 |    1.00 |    1.00 |    4.00 |    5.00 |    5.00 | ▇▁▁▂▇ |
| route9             |       1840 |           0.01 |    2.54 | 1.17 |    2.00 |    2.00 |    2.00 |    2.00 |    5.00 | ▇▁▁▁▂ |
| route10            |       1845 |           0.01 |    3.00 | 0.00 |    3.00 |    3.00 |    3.00 |    3.00 |    3.00 | ▁▁▇▁▁ |
| route11            |       1845 |           0.01 |    7.00 | 0.00 |    7.00 |    7.00 |    7.00 |    7.00 |    7.00 | ▁▁▇▁▁ |

``` r
names(nyc_df)
```

    ##  [1] "line"              "station_name"      "station_latitude" 
    ##  [4] "station_longitude" "entry"             "route1"           
    ##  [7] "route2"            "route3"            "route4"           
    ## [10] "route5"            "route6"            "route7"           
    ## [13] "route8"            "route9"            "route10"          
    ## [16] "route11"           "vending"           "entrance_type"    
    ## [19] "ada"

This dataset contains information from the NYC Transit data; in
particular, it has information related to each entrance and exit for
each subway station in NYC. There are 19 variables in this dataset:
line, station\_name, station\_latitude, station\_longitude, entry,
route1, route2, route3, route4, route5, route6, route7, route8, route9,
route10, route11, vending, entrance\_type, ada. For data cleaning steps,
I clean the variables’ name first. Then I select important variables and
drop unnecessary variables. After that I convert variable entry from
character to logical data type.

The rows of the dataset is 1868 and the columns of the dataset is 19.
The dimention is 1868 \* 19.

The data is not super tidy since we need to reformat variables route1 to
route11.

``` r
distinct_data1 = distinct(nyc_df, line, station_name, .keep_all = TRUE) 
nrow(distinct_data1)
```

    ## [1] 465

There are 465 distinct stations here.

``` r
  filter(distinct_data1,  ada == "TRUE") %>% 
  nrow()
```

    ## [1] 84

There are 84 stations are ADA compliant.

``` r
filter(nyc_df,  vending == "NO")
```

    ## # A tibble: 183 x 19
    ##    line  station_name station_latitude station_longitu… entry route1 route2
    ##    <chr> <chr>                   <dbl>            <dbl> <lgl> <chr>  <chr> 
    ##  1 4 Av… 53rd St                  40.6            -74.0 FALSE R      <NA>  
    ##  2 4 Av… 77th St                  40.6            -74.0 FALSE R      <NA>  
    ##  3 4 Av… 9th St                   40.7            -74.0 TRUE  F      G     
    ##  4 4 Av… Bay Ridge Av             40.6            -74.0 FALSE R      <NA>  
    ##  5 42nd… Grand Centr…             40.8            -74.0 FALSE GS     4     
    ##  6 42nd… Grand Centr…             40.8            -74.0 FALSE GS     4     
    ##  7 6 Av… 7th Av                   40.7            -74.0 TRUE  F      <NA>  
    ##  8 6 Av… 7th Av                   40.7            -74.0 TRUE  F      <NA>  
    ##  9 6 Av… 7th Av                   40.7            -74.0 TRUE  F      <NA>  
    ## 10 6 Av… 7th Av                   40.7            -74.0 TRUE  F      <NA>  
    ## # … with 173 more rows, and 12 more variables: route3 <chr>, route4 <chr>,
    ## #   route5 <chr>, route6 <chr>, route7 <chr>, route8 <dbl>, route9 <dbl>,
    ## #   route10 <dbl>, route11 <dbl>, vending <chr>, entrance_type <chr>, ada <lgl>

``` r
a = nrow(filter(nyc_df,  vending == "NO")) ##the number of station entrances / exits without vending
b = nrow(filter(nyc_df,  vending == "NO", entry == "TRUE")) ##the number of stations entrances / exits that allow entrance but do not have vending
b/a ##The proportion of station entrances / exits without vending allow entrance
```

    ## [1] 0.3770492

The proportion of station entrances / exits without vending allow
entrance is 0.3770492.

convert variable route9-route11 from double to char.

``` r
nyc_df_char = mutate(nyc_df, route8 = as.character(route8),
                route9 = as.character(route9),
                route10 = as.character(route10),
                route11 = as.character(route11)
                )
```

reform the dataset.

``` r
nyc_df_tidy =
  nyc_df_char %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  )
```

``` r
distinct_data2 = distinct(nyc_df_tidy, line, station_name, .keep_all = TRUE)
filter(distinct_data2,  route_name == "A") %>% 
  nrow()
```

    ## [1] 60

There are 60 distinct station.

``` r
filter(distinct_data2,  route_name == "A", ada == "TRUE") %>% 
  nrow()
```

    ## [1] 17

There are 17 ada compliant of the stations serve the A train.

## Problem 3

read the csv datasets

``` r
pols_month_df = 
  read_csv("./data/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, 
           into = c("year", "month", "day"),
           convert = TRUE
           ) %>% 
 mutate(
   month = month.abb[as.factor(month)],
   president = case_when(
     prez_gop == 1 ~ 'gop',
     prez_dem == 1 ~ 'dem'
   ) 
   ) %>% 
  arrange(year, month) %>%
  select(-prez_dem, -prez_gop, -day)
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

clean snap data

``` r
snap_df = 
read_csv("./data/snp.csv") %>% 
  janitor::clean_names() %>% 
 separate(date, 
          into = c("month", "day","year"),
          sep = "/",
          convert = TRUE
          ) %>% 
 mutate( month = month.abb[as.factor(month)]) %>% 
  arrange(year, month) %>% 
  select(year, month, close)
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

clean the unemployment data

``` r
unemployment_df = 
  read_csv("./data/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) %>% 
  mutate( month = month.abb[as.factor(month)])
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

merge the files

``` r
merge_pols_snap = left_join(pols_month_df, snap_df, by = c("year","month"))
merge_all =left_join(merge_pols_snap, unemployment_df, by = c("year","month"))
```
